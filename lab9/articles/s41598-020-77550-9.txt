Computational modeling of human reasoning processes for interpretable visual knowledge: a case study with radiographers
Visual reasoning is critical in many complex visual tasks in medicine such as radiology or pathology. It is challenging to explicitly explain reasoning processes due to the dynamic nature of real-time human cognition. A deeper understanding of such reasoning processes is necessary for improving diagnostic accuracy and computational tools. Most computational analysis methods for visual attention utilize black-box algorithms which lack explainability and are therefore limited in understanding the visual reasoning processes. In this paper, we propose a computational method to quantify and dissect visual reasoning. The method characterizes spatial and temporal features and identifies common and contrast visual reasoning patterns to extract significant gaze activities. The visual reasoning patterns are explainable and can be compared among different groups to discover strategy differences. Experiments with radiographers of varied levels of expertise on 10 levels of visual tasks were conducted. Our empirical observations show that the method can capture the temporal and spatial features of human visual attention and distinguish expertise level. The extracted patterns are further examined and interpreted to showcase key differences between expertise levels in the visual reasoning processes. By revealing task-related reasoning processes, this method demonstrates potential for explaining human visual understanding.
Human visual processing is critical in the reasoning and decision making for many tasks and attracts researchers from various disciplines. Studies in psychology and neural science show that visual attention is heavily affected by both visual features and visual tasks1, as well as accumulated experience2. In computer science, people have been able to model and predict human visual attention regarding different levels of visual features3, emotion4,5, and viewing time6. However, many studies target simple visual tasks which cannot fully reveal the reasoning processes behind complex visual tasks.
One of the areas needing such understanding is medical image interpretation. Medical imaging specialists accumulate implicit knowledge of highly complicated visual tasks through years of experience with real cases, which makes them irreplaceable despite recent developments of computer vision and clinical decision support systems which heavily rely on black-box algorithms7,8. Lack of explainability is one major concern preventing these systems from being widely adopted in critical areas9, especially where the human reasoning processes are not well standardized such as radiology and pathology. For such complex tasks, the visual reasoning processes may differ depending on the task and level of expertise. In some cases, there can be diagnostic discrepancies among different experts or even between two diagnoses by the same expert10. One major source for error (around 60%) in radiology is the radiologists’ perception11,12. The main method for identifying the causes of the errors is case-by-case analysis in which experts need to closely examine and discuss the images with discrepancies13,14,15. There is an urgent need to understand and formalize the visual reasoning processes behind medical image interpretation, and an efficient computational method is essential to provide insight and evidence for this purpose16.
As an unobtrusive method that can be seamlessly integrated into natural workflow, eye movement analysis has been applied to medical diagnosis processes to investigate visual decision making17,18,19, model perceptual behavior20,21, and improve medical interpretation22,23. Previous studies show the applicability of eye movement in decoding visual attention, but some are limited by their methods which either require extensive analysis of entire eye movement sequences or lack quantified measurements to capture the temporal or spatial differences24. To this point, many studies have been undertaken to extract detailed reasoning processes, but the methods and metrics employed could only support identification of general visual behaviors supported by saliency maps or time consumption for fixations and saccades25.
There have been a number of eye movement analysis methods employed in the field of eye tracking studies26,27,28,29,30,31,32,33,34. Many methods focus on clustering similar eye movement sequences26,27,28,30, and some can produce explainable results that can be understood by medical specialists31,32,33,34. The extracted eye movement sequences from some of the methods with explainable results could be a high-level explanation abstracted from multiple sequences31,32,35. Given that some sequences could contain prolonged scanning and search periods and may not always focus on task-related regions, such results can potentially omit important details in subsequences. We identify a need for a new computational method to produce task-related, explainable visual reasoning patterns.
We propose a visual knowledge and reasoning discovery system to efficiently identify cases with significant differences and extract common and unique visual reasoning patterns. We adapt the Markov Chain (MC) and graph models to quantify the spatial and temporal differences of eye movement sequences. This provides a quantitative measure on how people differ in certain tasks and a reference for locating significant cases. The system also extracts common and unique patterns of eye movement exhibited frequently by all viewers or only in certain viewer groups. The extraction process filters out irrelevant or transitional fixations while focusing only on the prevalent eye movements which fulfil certain goals. The extracted patterns are original eye movement sequences which are explainable and can be translated into visual strategies. These patterns can support understanding the visual reasoning processes and provide data for evidence-based medical image interpretation. The method is a first step toward our effort to design computational methods with explainable reasoning processes.
The rest of the paper is structured as follows. In Method, we introduce the system architecture and explain the components of data preprocessing, visual reasoning quantification, and pattern extraction. The Results section shows the effectiveness of quantification in differentiating visual reasoning processes and explains the significant patterns with expertise progression. In Discussion, the results are summarized, and the prospects of the methods are discussed.
As explained the Method section, the system should help researchers effectively identify spatial and temporal differences in visual reasoning processes and provide significant common and contrast patterns for further examination. To test for effectiveness, an experiment was conducted to compare visual reasoning with subjects at varying expertise levels.
Informed consent was obtained from 39 subjects from two major viewer groups including 15 registered radiographers (experts, more than 10,000 h of clinical experience) and 24 students (novices) in the Radiography department at the University of Missouri. The novice group was recruited from the same bachelor-level program and further divided into 11 seniors (completed all image analysis coursework and over 1000 h of experience) and 13 juniors (less than 400 h, training in progress). The results are produced by comparing the eye movement recordings between the groups. An EyeTribe eye tracker41 with 60 Hz sampling rate was applied to collect the viewers’ eye movements. The questions and images were shown on a 27-inch monitor placed about 60 cm away from participants.
The experiment included 10 X-ray images with 10 questions. All images used in the study were obtained from the teaching files maintained by the Radiography department and had been previously de-identified to ensure patient privacy. The images used in this experiment contained no personally identifiable patient information and their use does not violate HIPAA regulations. Each image was chosen for a question in the modified 10-level visual structure42 (Table 1) which groups visual tasks from low syntactic levels to high semantic levels. The progression of difficulty imitates the normal reasoning process of a radiographer approaching a new image. The participants viewed the task question prior to initiating the 15 seconds of image viewing and eye movements recording. They were then asked to explain their answers and reasons which are recorded and transcribed. The experiment was approved by the University of Missouri Health Science Institutional Review Board (IRB #2001653), and all methods were performed in accordance with the relevant guidelines and regulations. The fixation statistics can be found in Supplementary document.
We tested our distance model on all viewer groups. Our hypothesis is that for a specific distance, there should be a high similarity with intragroup visual reasoning processes and a large difference for intergroup visual reasoning processes. We performed the following group comparisons: Expert vs. Expert (EE), Expert vs. Senior (ES), Expert vs. Junior (EJ) and Expert vs. Novice (EN). These comparisons were performed using all-against-all distance measurement between and within groups. The Expert vs. Expert (EE) distances allow us to establish the level of internal consistency within the expert group. The Mann-Whitney U test was used on combinations of these four group comparisons because it has no requirement on data distribution. A p-value less than 0.05 indicates that two distance distributions are statistically different. More details about the distances are shown in Supplementary document.
The statistical test results are shown in Table 2 and the significant cases are highlighted. For spatial distances, we give equal weight to each component. We can see that experts’ visual reasoning patterns are different from those of novices in all cases based on spatial distance. In the junior and senior groups, we can see that there are significant differences between junior and senior students’ gaze patterns compared to experts’ patterns at low-level tasks (Tasks 1–3). But for most high-level tasks (Tasks 4–10), senior and junior students perform similarly, which demonstrates the experience gap between novices and experts. Similar to the spatial distance testing, the Mann-Whitney U test was applied to the temporal distances. Compared to the spatial component results, we have similar observations from the experiments using the temporal component. For the novice group, junior students who have one less year of training are close in performance to the senior students for most of the tasks. This could be explained as the visiting order of ROIs may not be important for both novice student groups for image understanding. On the other hand, experts perform differently comparing to both junior and senior students. With well-trained knowledge, experts can quickly locate relevant regions in the images and perform quite differently from novice students in temporal visual activities.
With the significant cases highlighted by the two distance measurements, we explored the common and contrast visual reasoning patterns to investigate how the expertise groups differ in different tasks. We analyzed eye movement sequences with the following comparisons: Expert vs. Novice (EN), Expert vs. Senior (ES), Expert vs. Junior (EJ), and Senior vs. Junior (SJ). We set the minimal length of a subsequence to 4 ROIs (400 ms). To compensate for small variations caused by micro-saccade or stray fixations, we consider two subsequences similar if their Levenshtein distance is no greater than 2. The support threshold is set at 0.2, and the growth threshold is set to 2.
The eye movement subsequences identified can be characterized by the number of contrast patterns, average and maximum pattern lengths, average pair-wise Levenshtein and ROI distances, average support value, and percentage of pattern with infinite growth of EN comparison, as shown in Table 3. The number of patterns and pattern lengths vary depending on the task. Some tasks can be finished by checking different ROIs of the same clinical significance, which could result in fewer patterns. Most patterns are unique in one group with an infinity growth rate. The patterns also exhibit an average length of 7–9, indicating the capability of capturing meaningful visual reasoning patterns which can characterize an expertise group. Task 8 stands out with the fewest patterns and shortest lengths, indicating that the visual reasoning processes are highly diverse even within a group. The longer average pattern lengths for experts in most tasks reflect higher unity in their sequences. Also, the phenomenon in which experts’ patterns generally have smaller ROI distances than novices’ shows that the experts are more efficient by focusing on less ROIs. The larger Levenshtein distances for experts indicate that they develop different orders of ROI examination in practice which may not conform with textbook instructions followed by novices. The patterns, along with the think-aloud recordings, are examined and interpreted by an experienced radiographer to identify meaningful differences in visual reasoning behaviors.
Not only can the subsequence patterns reveal the visual reasoning common to an expertise group, they can also reveal details regarding the visual tasks undertaken. Tasks 1 to 3 only require fundamental radiology knowledge and concern basic image features like contrast and exposure, so the viewers might fixate on any region regardless of medical significance for the same visual information. At the medium levels, the contrast patterns and common patterns start to show correlations with expertise. Task 4 asks the viewer about the projection orientation while showing a picture of the right femoral head and pelvis (Fig. 2). Unlike the low-level tasks, this task requires a moderate amount of radiology and medical knowledge, and the viewer needs to visit certain ROIs. Additionally, the image is projected from the side of the pelvis, which obscures the structure of the femoral head and makes it less distinguishable than it is in other projections. This less common projection exposes the visual reasoning differences between expertise groups. All groups exhibit short common patterns (around 1350 ms) of examining the orientation of the femoral head with pelvis, but the average support is generally lower in experts than in novices (0.33 vs. 0.47). Longer patterns (above 2000 ms) only show up in novices’ sequences as contrast patterns. The prolonged fixations exhibited by the novices, especially juniors, reveal their lack of experience, and that additional examination is required. Most novices commented that they did not see anything abnormal. The experts exhibit several unique contrast patterns with extended fixations on the “R” marker (noted in red in Fig. 2a,b). The three dots at the bottom of the circle indicate that the image is being taken with a cross-table projection where the image receptor is standing up perpendicularly to the patient table. Seniors also show some contrast patterns that indicate learning progression when comparing the patterns against juniors. They have longer fixations on the greater and lesser trochanter, which tell seniors that the image is in a lateral projection with the x-ray beam traveling from the patient’s inner thigh through his outer thigh. The experts share some shorter fixations of trochanters with the seniors, but the patterns generally contain fixations on the “R” marker to confirm the two pieces of orientation information.
Example eye movement sequences for Task 4. ROIs are marked with yellow lines. The connected blue lines and circles on each image show an entire eye movement sequence by one participant. Each circle is a fixation in the sequence, and its size represents the fixation duration. The red subsequences highlight the contrast pattern by experts which checks the R marks and the label. The sequences are plotted with Matplotlib43.
As task complexity increases, the differences between expertise groups become more evident. The chest x-ray presented with Task 7, “Identify 3 foreign objects”, has complex features and objects (Fig. 3), and it exposes more interesting commonalities and differences. The tubes and clips across the patient’s chest are obvious foreign objects, and they appear in all expertise groups as common patterns with support values above 0.5. The juniors pay the most attention to the right humerus, where there are multiple wires and snaps (Fig. 3c). One contrast pattern shared only among the novices is the prolonged fixation (above 1050 ms) on the collimated edge where the x-ray exposure ends (Fig. 3c,d). The wedge-shaped line and shadow extending from the upper left corner of the image were created by a misaligned x-ray tube and image receptor. It indicates that this image was performed in the patient’s room. Such a situation rarely occurs in a dedicated x-ray room where most x-rays are performed. These misaligned images are novel to the novices, so the collimated edge attracts their visual attention, which is confirmed in their comments. However, the experts have become accustomed to this occurrence and do not focus on it. With extended experience and medical knowledge, the experts show contrast patterns which dwell on the hilum and the left lung (Fig. 3a,b). The cloudiness in the hilum and left lung area suggests either pneumonia or atelectasis. The image feature requires familiarity with chest anatomy so the pathology can be detected, and the contrast pattern of the expert exposes the key differences.
Example eye movement sequences for Task 7. The red subsequences of experts show close examination of the potential diseased area, while the novices are distracted by the technical imperfections at the left edge (green subsequences).
Increased task complexity also requires increases in dwell times and the ability to prioritize visual focus on relevant ROIs. Task 10 requires a high level of anatomy and radiology knowledge to answer, and these knowledge differences are evident in the patterns of each expertise group. When compared to novices, the experts have unique contrast patterns with much longer fixation sequences on the humeral head, humerus, and the area where humerus was impacted into humeral head (Fig. 4a,b). The extended dwell time (above 2400 ms) and constant cross examination indicates that they were examining the compression fracture of the humeral head, which is confirmed by mentioning traumatic injury by all experts. The common patterns novices share with experts show that they frequently look at the humerus and humeral head as well, although the average support is much lower (0.38 vs. 0.55). The compression fracture is not easily recognized and thus skimmed through by the novices because the outline of the bone remains basically normal. It is fine textural changes that the experts were targeting. However, the seniors show a higher support value of these shorter common patterns than juniors (0.55 vs. 0.44), suggesting that more seniors are starting to check critical areas. This is supported by some seniors’ comments on the trauma. The experience progression is also highlighted by some seniors’ unique patterns focused on an immobilizing sponge (Fig. 4c). The sponge is very similar in brightness and contrast to the surrounding tissues, making it less noticeable than high-contrast non-anatomical structures like the clip of the safety strap. The seniors have learned to ignore obvious distractions, but their attention is still captured by less familiar, but still irrelevant, objects. The contrast patterns for juniors, most of who commented mainly on unrecognized artifacts, show the greatest amount of distraction by non-anatomical structures such as backboard edge and handle (Fig. 4d). These non-anatomical structures are unusual and thus salient to novices, but they are limited in assisting with the task. For experts, their expertise is revealed by the high concentration of task-effective fixations in their contrast patterns.
Example eye movement sequences for Task 10. Unlike novices, the experts are not distracted by the non-anatomical structures and closely examine the fracture of the humeral head as shown by the red subsequences.
The extracted common and contrast patterns not only explain the spatial and temporal differences in visual reasoning processes, they also unveil subtle experience progression that can only be found in segments of the whole eye movement sequences. The visual reasoning explanations of the three cases are based on a portion of the extracted patterns in Table 3. The extracted patterns for other high-level tasks also demonstrate different reasoning processes between expertise groups, illustrating the potential for applying the method to more cases, as well as to other medical image areas, for a systematic and comprehensive understanding of human visual reasoning.
This paper reports a computational method that may bring the research community one step closer to designing explainable computational tools that can explain human visual reasoning. Previous studies have shown that experts perceive diagnostic-related regions more quickly44 and allocate their attention more efficiently45. Using the results from our method, we can go one step further and carefully examine significant segments of eye movement and the underlying anatomical meaning when comparing different tasks and groups of viewers. This method is especially powerful in cases where some visual reasoning processes are subconscious and cannot be fully explained. Such processes are prone to be overlooked when presented in a prolonged series of eye movements. Our system quantitatively characterizes visual reasoning processes to quickly highlight important cases, and the extracted common and contrast patterns can lead to new knowledge discovery.
Examining the spatial and temporal distance comparison results and extracted common and contrast patterns, we found that experts and novices tend to use different visual patterns to solve the same high-level tasks. In the novice group, we observed that junior and senior students have minimal significant differences when dealing with most mid- to high-level tasks based on the entire eye movement sequences. However, the contrast patterns show some small differences in subsequences when comparing the two novice groups, which indicate some progression in experience with seniors. This small difference is often overshadowed by the overall characteristics. It is only when the common and contrast patterns are analyzed that we are able to detect the detailed differences in visual reasoning processes between the groups.
In general, the spatial and temporal representations and distances allow us to quantitatively determine the statistical difference in visual behaviors. The extracted common and contrast patterns provide readable and explainable subsequences and enable the domain experts to investigate the visual strategies in detail. This retrospective study shows promise for discovering implicit visual strategies gained through experience and may lead to new, evidence-based knowledge. By capturing and explaining the steps involved in visual reasoning for specific tasks, we can better support those who engage regularly in complex visual reasoning. In the educational setting, it is valuable for formulating more efficient instruction, highlighting avoidable distractions, and improving professional practices. In healthcare delivery, it holds promise for reducing diagnostic discrepancies and improving health outcomes.
We believe that our attempt to use computational means to quantify and understand the human visual reasoning process may provide the community with a starting point to systematically capture the implicit visual reasoning processes. The capability of a system with explainable results would bring potentially valuable and novel insights in visual strategies to fields that rely on image-based diagnosis such as dermatology, pathology and radiology. It will also allow us to gain potential direction for designing explainable artificial intelligence algorithms based on accumulated human knowledge that is supported by the evidence in the recorded visual attention of experts. In the future, the distances can be utilized to enhance content-based medical image retrieval systems46 to search and retrieve similar visual reasoning sequences given a case or an image. The system can also be adapted for other visually-intensive tasks such as geospatial information retrieval47.
