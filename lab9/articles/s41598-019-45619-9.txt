α-Rank: Multi-Agent Evaluation by Evolution
We introduce α-Rank, a principled evolutionary dynamics methodology, for the evaluation and ranking of agents in large-scale multi-agent interactions, grounded in a novel dynamical game-theoretic solution concept called Markov-Conley chains (MCCs). The approach leverages continuous-time and discrete-time evolutionary dynamical systems applied to empirical games, and scales tractably in the number of agents, in the type of interactions (beyond dyadic), and the type of empirical games (symmetric and asymmetric). Current models are fundamentally limited in one or more of these dimensions, and are not guaranteed to converge to the desired game-theoretic solution concept (typically the Nash equilibrium). α-Rank automatically provides a ranking over the set of agents under evaluation and provides insights into their strengths, weaknesses, and long-term dynamics in terms of basins of attraction and sink components. This is a direct consequence of the correspondence we establish to the dynamical MCC solution concept when the underlying evolutionary model’s ranking-intensity parameter, α, is chosen to be large, which exactly forms the basis of α-Rank. In contrast to the Nash equilibrium, which is a static solution concept based solely on fixed points, MCCs are a dynamical solution concept based on the Markov chain formalism, Conley’s Fundamental Theorem of Dynamical Systems, and the core ingredients of dynamical systems: fixed points, recurrent sets, periodic orbits, and limit cycles. Our α-Rank method runs in polynomial time with respect to the total number of pure strategy profiles, whereas computing a Nash equilibrium for a general-sum game is known to be intractable. We introduce mathematical proofs that not only provide an overarching and unifying perspective of existing continuous- and discrete-time evolutionary evaluation models, but also reveal the formal underpinnings of the α-Rank methodology. We illustrate the method in canonical games and empirically validate it in several domains, including AlphaGo, AlphaZero, MuJoCo Soccer, and Poker.
This paper introduces a principled, practical, and descriptive methodology, which we call α-Rank. α-Rank enables evaluation and ranking of agents in large-scale multi-agent settings, and is grounded in a new game-theoretic solution concept, called Markov-Conley chains (MCCs), which captures the dynamics of multi-agent interactions. While much progress has been made in learning for games such as Go1,2 and Chess3, computational gains are now enabling algorithmic innovations in domains of significantly higher complexity, such as Poker4 and MuJoCo soccer5 where ranking of agents is much more intricate than in classical simple matrix games. With multi-agent learning domains of interest becoming increasingly more complex, we need methods for evaluation and ranking that are both comprehensive and theoretically well-grounded.
Evaluation of agents in a multi-agent context is a hard problem due to several complexity factors: strategy and action spaces of players quickly explode (e.g., multi-robot systems), models need to be able to deal with intransitive behaviors (e.g., cyclical best-responses in Rock-Paper-Scissors, but at a much higher scale), the number of agents can be large in the most interesting applications (e.g., Poker), types of interactions between agents may be complex (e.g., MuJoCo soccer), and payoffs for agents may be asymmetric (e.g., a board-game such as Scotland Yard).
This evaluation problem has been studied in Empirical Game Theory using the concept of empirical games or meta-games, and the convergence of their dynamics to Nash equilibria6,7,8,9. In Empirical Game Theory a meta-game is an abstraction of the underlying game, which considers meta-strategies rather than primitive actions6,8. In the Go domain, for example, meta-strategies may correspond to different AlphaGo agents (e.g., each meta-strategy is an agent using a set of specific training hyperparameters, policy representations, and so on). The players of the meta-game now have a choice between these different agents (henceforth synonymous with meta-strategies), and payoffs in the meta-game are calculated corresponding to the win/loss ratio of these agents against each other over many rounds of the full game of Go. Meta-games, therefore, enable us to investigate the strengths and weaknesses of these agents using game-theoretic evaluation techniques.
Existing meta-game analysis techniques, however, are still limited in a number of ways: either a low number of players or a low number of agents (i.e., meta-strategies) may be analyzed6,7,8,10. Specifically, on the one hand continuous-time meta-game evaluation models, using replicator dynamics from Evolutionary Game Theory11,12,13,14,15, are deployed to capture the micro-dynamics of interacting agents. These approaches study and visualize basins of attraction and equilibria of interacting agents, but are limited as they can only be feasibly applied to games involving few agents, exploding in complexity in the case of large and asymmetric games. On the other hand, existing discrete-time meta-game evaluation models (e.g.16,17,18,19,20) capture the macro-dynamics of interacting agents, but involve a large number of evolutionary parameters and are not yet grounded in a game-theoretic solution concept.
To further compound these issues, using the Nash equilibrium as a solution concept for meta-game evaluation in these dynamical models is in many ways problematic: first, computing a Nash equilibrium is computationally difficult21,22; second, there are intractable equilibrium selection issues even if Nash equilibria can be computed23,24,25; finally, there is an inherent incompatibility in the sense that it is not guaranteed that dynamical systems will converge to a Nash equilibrium26,27, or, in fact, to any fixed point. However, instead of taking this as a disappointing flaw of dynamical systems models, we see it as an opportunity to look for a novel solution concept that does not have the same limitations as Nash in relation to these dynamical systems. Specifically, exactly as J. Nash used one of the most advanced topological results of his time, i.e., Kakutani’s fixed point theorem28, as the basis for the Nash solution concept, in the present work, we employ Conley’s Fundamental Theorem of Dynamical Systems29 and propose the solution concept of Markov-Conley chains (MCCs). Intuitively, Nash is a static solution concept solely based on fixed points. MCCs, by contrast, are a dynamic solution concept based not only on fixed points, but also on recurrent sets, periodic orbits, and limit cycles, which are fundamental ingredients of dynamical systems. The key advantages are that MCCs comprehensively capture the long-term behaviors of our (inherently dynamical) evolutionary systems, and our associated α-Rank method runs in polynomial time with respect to the total number of pure strategy profiles (whereas computing a Nash equilibrium for a general-sum game is PPAD-complete21).
In the following we summarize our generalized ranking model and the main theoretical and empirical results. We start by outlining how the α-Rank procedure exactly works. Then we continue with illustrating α-Rank in a number of canonical examples. We continue with some deeper understanding of α-Rank’s evolutionary dynamics model by introducing some further intuitions and theoretical results, and we end with an empirical validation of α-Rank in various domains.
We first detail the α-Rank algorithm, then provide some insights and intuitions to further facilitate the understanding of our ranking method and solution concept.
Based on the dynamical concepts of chain recurrence and MCCs established, we now detail a descriptive method, titled α-Rank, for computing strategy rankings in a multi-agent interaction:
Construct the meta-game payoff table \({M}^{k}\) for each population \(k\) from data of multi-agent interactions, or from running game simulations.
Compute the transition matrix \(C\) as outlined in Section 2.1.4. Per the discussions in Section 2.5, one must use a sufficiently large ranking-intensity value \(\alpha \) in (4); this ensures that \(\alpha \)-Rank preserves the ranking of strategies with closest correspondence to the MCC solution concept. Note that setting \(\alpha \) arbitrarily high can result in numerical issues that make the representation of the Markov chain used in simulations reducible. As a large enough value is dependent on the domain under study, a useful heuristic is to conduct a sweep over \(\alpha \), starting from a small value and increasing it exponentially until convergence of rankings.
Compute the unique stationary distribution, \(\pi \), of transition matrix \(C\). Each element of the stationary distribution corresponds to the time the populations spend in a given strategy profile.
Compute the agent rankings, which correspond to the ordered masses of the stationary distribution \(\pi \). The stationary distribution mass for each agent constitutes a ‘score’ for it (as might be shown, e.g., on a leaderboard).
In this section, we elaborate on the differences between the Nash and MCC solution concepts. Our notion of a ‘solution concept’, informally, corresponds to a description of how agents will play a game. The MCC solution concept is not based on the idea of individual rationality, such as in Nash, but is rather biologically-conditioned, such as considered in evolutionary game theory11,13. As such, our solution concept of MCCs can be seen as a descriptive approach (in the sense of  58) or predictive approach (in the sense of10), providing an understanding of the underlying dynamic behaviors as well as an understanding of what these behaviors converge to in the long-term. This is also where traditional game theory differs from evolutionary game theory, in the sense that the former is normative and tells players how to play, while the latter is descriptive and relaxes some of the strong assumptions underpinning the Nash equilibrium concept.
We note that Nash has done double duty in game theory and remains a very important concept in multi-agent systems research. However, besides classical game theory making strong assumptions regarding the rationality of players involved in the interaction, there exist many fundamental limitations with the concept of a Nash equilibrium: intractability (computing a Nash is PPAD-complete), equilibrium selection, and the incompatibility of this static concept with the dynamic behaviors of agents in interacting systems. To compound these issues, even methods that aim to compute an approximate Nash are problematic: a typical approach is to use exploitability to measure deviation from Nash and as such use it as a method to closely approximate one; the problem with this is that it is also intractable for large games (typically the ones we are interested in), and there even still remain issues with using exploitability as a measure of strategy strength (e.g., see59). Overall, there seems little hope of deploying the Nash equilibrium as a solution concept for the evaluation of agents in general large-scale (empirical) games.
The concept of an MCC, by contrast, embraces the dynamical systems perspective, in a manner similar to evolutionary game theory. Rather than trying to capture the strategic behavior of players in an equilibrium, we deploy a dynamical system based on the evolutionary interactions of agents that captures and describes the long-term behavior of the players involved in the interaction. As such, our approach is descriptive rather than prescriptive, in the sense that it is not prescribing the strategies that one should play; rather, our approach provides useful information regarding the strategies that are evolutionarily non-transient (i.e., resistant to mutants), and highlights the remaining strategies that one might play in practice. To understand MCCs requires a shift away from the classical models described above for games and multi-agent interactions. Our new paradigm is to allow the dynamics to roll out and enable strong (i.e., non-transient) agents to emerge and weak (i.e., transient) agents to vanish naturally through their long-term interactions. The resulting solution concept not only permits an automatic ranking of agents’ evolutionary strengths, but is powerful both in terms of computability and usability: our rankings are guaranteed to exist, can be computed tractably for any game, and involve no equilibrium selection issues as the evolutionary process converges to a unique stationary distribution. Nash tries to identify static single points in the simplex that capture simultaneous best response behaviors of agents, but comes with the range of complications mentioned above. On the other hand, the support of our stationary distribution captures the strongest non-transient agents, which may be interchangeably played by interacting populations and therefore constitute a dynamic output of our approach.
Given that both Nash and MCCs share a common foundation in the notion of a best response (i.e., simultaneous best responses for Nash, and the sink components of a best response graph for MCCs), it is interesting to consider the circumstances under which the two concepts coincide. There do, indeed, exist such exceptional circumstances: for example, for a potential game, every better response sequence converges to a (pure) Nash equilibrium, which coincides with an MCC. However, even in relatively simple games, differences between the two solution concepts are expected to occur in general due to the inherently dynamic nature of MCCs (as opposed to Nash). For example, in the Biased Rock-Paper-Scissors game detailed in Section 3.2.2, the Nash equilibrium and stationary distribution are not equivalent due to the cyclical nature of the game; each player’s symmetric Nash is \((\frac{1}{16},\frac{5}{8},\frac{5}{16})\), whereas the stationary distribution is \((\frac{1}{3},\frac{1}{3},\frac{1}{3})\). The key difference here is that whereas Nash is prescriptive and tells players which strategy mixture to use, namely \((\frac{1}{16},\frac{5}{8},\frac{5}{16})\) assuming rational opponents, \(\alpha \)-Rank is descriptive in the sense that it filters out evolutionary transient strategies and yields a ranking of the remaining strategies in terms of their long-term survival. In the Biased Rock-Paper-Scissors example, \(\alpha \)-Rank reveals that all three strategies are equally likely to persist in the long-term as they are part of the same sink strongly connected component of the response graph. In other words, the stationary distribution mass (i.e., the \(\alpha \)-Rank score) on a particular strategy is indicative of its resistance to being invaded by any other strategy, including those in the distribution support. In the case of the Biased Rock-Paper-Scissors game, this means that the three strategies are equally likely to be invaded by a mutant, in the sense that their outgoing fixation probabilities are equivalent. In contrast to our evolutionary ranking, Nash comes without any such stability properties (e.g., consider the interior mixed Nash in Fig. 4b). Even computing Evolutionary Stable Strategies (ESS)13, a refinement of Nash equilibria, is intractable60,61. In larger games (e.g., AlphaZero in Section 3.4.2), the reduction in the number of agents that are resistant to mutations is more dramatic (in the sense of the stationary distribution support size being much smaller than the total number of agents) and less obvious (in the sense that more-resistant agents are not always the ones that have been trained for longer). In summary, the strategies chosen by our approach are those favored by evolutionary selection, as opposed to the Nash strategies, which are simultaneous best-responses.
We revisit the earlier conceptual examples of Rock-Paper-Scissors and Battle of the Sexes from Section 2.2 to illustrate the rankings provided by the \(\alpha \)-Rank methodology. We use a population size of \(m=50\) in our evaluations.
In the Rock-Paper-Scissors game, recall the cyclical nature of the discrete-time Markov chain (shown in Fig. 6a) for a fixed value of ranking-intensity parameter, \(\alpha \). We first investigate the impact of the ranking-intensity on overall strategy rankings, by plotting the stationary distribution as a function of \(\alpha \) in Fig. 6b. The result is that the population spends \(\frac{1}{3}\) of its time playing each strategy regardless of the value of \(\alpha \), which is in line with intuition due to the cyclical best-response structure of the game’s payoffs. The Nash equilibrium, for comparison, is also \((\frac{1}{3},\frac{1}{3},\frac{1}{3})\). The \(\alpha \)-Rank output Fig. 6c, which corresponds to a high value of \(\alpha \), thus indicates a tied ranking for all three strategies, also in line with intuition.
Rock-Paper-Scissors game. (a) Discrete-time dynamics. (b) Ranking-intensity sweep. (c) α-Rank results.
Consider now the game of Rock-Paper-Scissors, but with biased payoffs (shown in Fig. 7a). The introduction of the bias moves the Nash from the center of the simplex towards one of the corners, specifically \((\frac{1}{16},\frac{5}{8},\frac{5}{16})\) in this case. It is worthwhile to investigate the corresponding variation of the stationary distribution masses as a function of the ranking-intensity \(\alpha \) (Fig. 7c) in this case. As evident from the fixation probabilities (13) of the generalized discrete-time model, very small values of \(\alpha \) cause the raw values of payoff to have a very low impact on the dynamics captured by discrete-time Markov chain; in this case, any mutant strategy has the same probability of taking over the population, regardless of the current strategy played by the population. This corresponds well to Fig. 7c, where small \(\alpha \) values yield stationary distributions close to \(\pi =(\frac{1}{3},\frac{1}{3},\frac{1}{3})\).
Biased Rock-Paper-Scissors game. (a) Payoff matrix. (b) Discrete-time dynamics. (c) Ranking-intensity sweep. (d) α-Rank results.
As \(\alpha \) increases, payoff values play a correspondingly more critical role in dictating the long-term population state; in Fig. 7c, the population tends to play Paper most often within this intermediate range of \(\alpha \). Most interesting to us, however, is the case where \(\alpha \) increases to the point that our discrete-time model bears a close correspondence to the MCC solution concept (per Theorem 2.5.1). In this limit of large \(\alpha \), the striking outcome is that the stationary distribution once again converges to \((\frac{1}{3},\frac{1}{3},\frac{1}{3})\). Thus, \(\alpha \)-Rank yields the high-level conclusion that in the long term, a monomorphic population playing any of the 3 given strategies can be completely and repeatedly displaced by a rare mutant, and as such assigns the same ranking to all strategies (Fig. 7d). This simple example illustrates perhaps the most important trait of the MCC solution concept and resulting \(\alpha \)-Rank methodology: they capture the fundamental dynamical structure of games and long-term intransitivities that exist therein, with the rankings produced corresponding to the dynamical strategy space consumption or basins of attraction of strategies.
We consider next an example of \(\alpha \)-Rank applied to an asymmetric game – the Battle of the Sexes. Figure 8b plots the stationary distribution against ranking-intensity \(\alpha \), where we again observe a uniform stationary distribution corresponding to very low values of \(\alpha \). As \(\alpha \) increases, we observe the emergence of two sink chain components corresponding to strategy profiles \((O,O)\) and \((M,M)\), which thus attain the top \(\alpha \)-Rank scores in Fig. 8c. Note the distinct convergence behaviors of strategy profiles \((O,M)\) and \((M,O)\) in Fig. 8b, where the stationary distribution mass on the \((M,O)\) converges to \(0\) faster than that of \((O,M)\) for an increasing value of \(\alpha \). This is directly due to the structure of the underlying payoffs and the resulting differences in fixation probabilities. Namely, starting from profile \((M,O)\), if either player deviates, that player increases their local payoff from 0 to 3. Likewise, if either player deviates starting from profile \((O,M)\), that player’s payoff increases from \(0\) to \(2\). Correspondingly, the fixation probabilities out of \((M,O)\) are higher than those out of \((O,M)\) (Fig. 8a), and thus the stationary distribution mass on \((M,O)\) converges to \(0\) faster than that of \((O,M)\) as \(\alpha \) increases. We note that these low-\(\alpha \) behaviors, while interesting, have no impact on the final rankings computed in the limit of large \(\alpha \) (Fig. 8c). We refer the interested reader to62 for a detailed analysis of the non-coordination components of the stationary distribution in mutualistic interactions, such as the Battle of the Sexes.
Battle of the Sexes. (a) Discrete-time dynamics (see (c) for node-wise scores corresponding to stationary distribution masses). (b) Ranking-intensity sweep. (c) α-Rank results.
We conclude this discussion by noting that despite the asymmetric nature of the payoffs in this example, the computational techniques used by \(\alpha \)-Rank to conduct the evaluation are essentially identical to the simpler (symmetric) Rock-Paper-Scissors game. This key advantage is especially evident in contrast to recent evaluation approaches that involve decomposition of a asymmetric game into multiple counterpart symmetric games, which must then be concurrently analyzed7.
This section presents key properties related to the structure of the underlying discrete-time model used in \(\alpha \)-Rank, and computational complexity of the ranking analysis.
(Structure of \(C\)). Given strategy profile \({s}_{i}\) corresponding to row \(i\) of \(C\), the number of valid profiles it can transition to is \(1+{\sum }_{k}\,(|{S}^{k}|-\mathrm{1)}\) (i.e., either \({s}_{i}\) self-transitions, or one of the populations \(k\) switches to a different monomorphic strategy). The sparsity of \(C\) is then,
Therefore, for games involving many players and strategies, transition matrix \(C\) is large (in the sense that there exist \(|S|\) states), but extremely sparse (in the sense that there exist only \(1+{\sum }_{k}\,(|{S}^{k}|-\mathrm{1)}\) outgoing edges from each state). For example, in a 6-wise interaction game where agents in each population have a choice over 4 strategies, \(C\) is 99.53% sparse.
(Computational complexity of solving for \(\pi \)). The sparse structure of the Markov transition matrix \(C\) (as identified in Property 3.3.1) can be exploited to solve for the stationary distribution \(\pi \) efficiently; specifically, computing the stationary distribution can be formulated as an eigenvalue problem, which can be computed in cubic-time in the number of total pure strategy profiles. The \(\alpha \)-Rank method is, therefore, tractable, in the sense that it runs in polynomial time with respect to the total number of pure strategies. This yields a major computational advantage, in stark contrast to conducting rankings by solving for Nash (which is PPAD-complete for general-sum games21, which our meta-games may be).
In this section we provide a series of experimental illustrations of \(\alpha \)-Rank in a varied set of domains, including AlphaGo, AlphaZero Chess, MuJoCo Soccer, and both Kuhn and Leduc Poker. As evident in Table 1, the analysis conducted is extensive across multiple axes of complexity, as the domains considered include symmetric and asymmetric games with different numbers of populations and ranges of strategies.
In this example we conduct an evolutionary ranking of AlphaGo agents based on the data reported in1. The meta-game considered here corresponds to a 2-player symmetric NFG with 7 AlphaGo agents: \(AG(r)\), \(AG(p)\), \(AG(v)\), \(AG(rv)\), \(AG(rp)\), \(AG(vp)\), and \(AG(rvp)\), where \(r\), \(v\), and \(p\) respectively denote the combination of rollouts, value networks, and/or policy networks used by each variant. The corresponding payoffs are the win rates for each pair of agent match-ups, as reported in Table 9 of1.
In Fig. 9c we summarize the rankings of these agents using the \(\alpha \)-Rank method. \(\alpha \)-Rank is quite conclusive in the sense that the top agent, \(AG(rvp)\), attains all of the stationary distribution mass, dominating all other agents. Further insights into the pairwise agent interactions are revealed by visualizing the underlying Markov chain, shown in Fig. 9a. Here the population flows (corresponding to the graph edges) indicate which agents are more evolutionarily viable than others. For example, the edge indicating flow from \(AG(r)\) to \(AG(rv)\) indicates that the latter agent is stronger in the short-term of evolutionary interactions. Moreover, the stationary distribution (corresponding to high \(\alpha \) values in Fig. 9b) reveals that all agents but \(AG(rvp)\) are transient in terms of the long-term dynamics, as a monomorphic population starting from any other agent node eventually reaches \(AG(rvp)\). In this sense, node \(AG(rvp)\) constitutes an evolutionary stable strategy. We also see in Fig. 9a that no cyclic behaviors occur in these interactions. Finally, we remark that the recent work of6 also conducted a meta-game analysis on these particular AlphaGo agents and drew similar conclusions to ours. The key limitation of their approach is that it can only directly analyze interactions between triplets of agents, as they rely on visualization of the continuous-time evolutionary dynamics on a 2-simplex. Thus, to draw conclusive results regarding the interactions of the full set of agents, they must concurrently conduct visual analysis of all possible 2-simplices (35 total in this case). This highlights a key benefit of \(\alpha \)-Rank as it can succinctly summarize agent evaluations with minimal intermediate human-in-the-loop analysis.
AlphaGo (Nature dataset). (a) Discrete-time dynamics. (b) Ranking-intensity sweep. (c) α-Rank results.
AlphaZero is a generalized algorithm that has been demonstrated to master the games of Go, Chess, and Shogi without reliance on human data3. Here we demonstrate the applicability of the \(\alpha \)-Rank evaluation method to large-scale domains by considering the interactions of a large number of AlphaZero agents playing the game of chess. In AlphaZero, training commences by randomly initializing the parameters of a neural network used to play the game in conjunction with a general-purpose tree search algorithm. To synthesize the corresponding meta-game, we take a ‘snapshot’ of the network at various stages of training, each of which becomes an agent in our meta-game. For example, agent \(AZ\mathrm{(27.5)}\) corresponds to a snapshot taken at approximately 27.5% of the total number of training iterations, while \(AZ\mathrm{(98.7)}\) corresponds to one taken approximately at the conclusion of training. We take 56 of these snapshots in total. The meta-game considered here is then a symmetric \(2\)-player NFG involving 56 agents, with payoffs again corresponding to the win-rates of every pair of agent match-ups. We note that there exist 27720 total simplex 2-faces in this dataset, substantially larger than those investigated in6, which quantifiably justifies the computational feasibility of our evaluation scheme.
We first analyze the evolutionary strengths of agents over a sweep of ranking-intensity \(\alpha \) (Fig. 10b). While the overall rankings are quite invariant to the value of \(\alpha \), we note again that a large value of \(\alpha \) dictates the final \(\alpha \)-Rank evaluations attained in Fig. 10c. To gain further insight into the inter-agent interactions, we consider the corresponding discrete-time evolutionary dynamics shown in Fig. 10a. Note that these interactions are evaluated using the entire 56-agent dataset, though visualized only for the top-ranked agents for readability. The majority of top-ranked agents indeed correspond to snapshots taken near the end of AlphaZero training (i.e., the strongest agents in terms of training time). Specifically, \(AZ\mathrm{(99.4)}\), which is the final snapshot in our dataset and thus the most-trained agent, attains the top rank with a score of 0.39, in contrast to the second-ranked \(AZ\mathrm{(93.9)}\) agent’s score of 0.22. This analysis does reveal some interesting outcomes, however: agent \(AZ\mathrm{(86.4)}\) is not only ranked 5-th overall, but also higher than several agents with longer training time, including \(AZ\mathrm{(88.8)}\), \(AZ\mathrm{(90.3)}\), and \(AZ\mathrm{(93.3)}\).
AlphaZero dataset. (a) Discrete-time dynamics. (b) Ranking-intensity sweep. (c) α-Rank results.
We also investigate here the relationship between the \(\alpha \)-Rank scores and Nash equilibria. A key point to recall is the equilibrium selection problem associated with Nash, as multiple equilibria can exist even in the case of two-player zero-sum meta-games. In the case of zero-sum meta-games, Balduzzi et al. show that there exists a unique maximum entropy (maxent) Nash equilibrium63, which constitutes a natural choice that we also use in the below comparisons. For general games, unfortunately, this selection issue persists for Nash, whereas it does not for \(\alpha \)-Rank due to the uniqueness of the associated ranking (see Theorem 2.1.2).
We compare the \(\alpha \)-Rank scores and maxent Nash by plotting each throughout AlphaZero training in Fig. 11a,b, respectively; we also plot their difference in Fig. 11c. At a given training iteration, the corresponding horizontal slice in each plot visualizes the associated evaluation metric (i.e., \(\alpha \)-Rank, maxent Nash, or difference of the two) computed for all agent snapshots up to that iteration. We first note that both evaluation methods reach a consensus that the strengths of AlphaZero agents generally increase with training, in the sense that only the latest agent snapshots (i.e., the ones closest to the diagonal) appear in the support of both \(\alpha \)-Rank scores and Nash. An interesting observation is that less-trained agents sometimes reappear in the support of the distributions as training progresses; this behavior may even occur multiple times for a particular agent.
AlphaZero (chess) agent evaluations throughout training. (a) α-Score vs. Training Time. (b) Maximum Entropy Nash vs. Training Time. (c) α-Score - Maximum Entropy Nash difference.
We consider also the quantitative similarity of \(\alpha \)-Rank and Nash in this domain. Figure 11c illustrates that differences do exist in the sense that certain agents are ranked higher via one method compared to the other. More fundamentally, however, we note a relationship exists between \(\alpha \)-Rank and Nash in the sense that they share a common rooting in the concept of best-response: by definition, each player’s strategy in a Nash equilibrium is a best response to the other players’ strategies; in addition, \(\alpha \)-Rank corresponds to the MCC solution concept, which itself is derived from the sink strongly-connected components of the game’s response graph. Despite the similarities, \(\alpha \)-Rank is a more refined solution concept than Nash in the sense that it is both rooted in dynamical systems and a best-response approach, which not only yields rankings, but also the associated dynamics graph (Fig. 10a) that gives insights into the long-term evolutionary strengths of agents. Beyond this, the critical advantage of \(\alpha \)-Rank is its tractability for general-sum games (per Property 3.3.2), as well as lack of underlying equilibrium selection issues; in combination, these features yield a powerful empirical methodology with little room for user confusion or interpretability issues. This analysis reveals fundamental insights not only in terms of the benefits of using \(\alpha \)-Rank to evaluate agents in a particular domain, but also an avenue of future work in terms of embedding the evaluation methodology into the training pipeline of agents involved in large and general games.
We consider here a dataset consisting of complex agent interactions in the continuous-action domain of MuJoCo soccer5. Specifically, this domain involves a multi-agent soccer physics-simulator environment with teams of 2 vs. 2 agents in the MuJoCo physics engine64. Each agent, specifically, uses a distinct variation of algorithmic and/or policy parameterization component (see5 for agent specifications). The underlying meta-game is a \(2\)-player NFG consisting of 10 agents, with payoffs corresponding to Fig. 2 of5.
We consider again the variation of the stationary distribution as a function of ranking-intensity \(\alpha \) (Fig. 12b). Under the large \(\alpha \) limit, only 6 agent survive, with the remaining 4 agents considered transient in the long-term. Moreoever, the top 3 \(\alpha \)-Ranked agents (\(C\), \(A\), and \(B\), as shown in Fig. 12c) correspond to the strongest agents highlighted in5, though \(\alpha \)-Rank highlights 3 additional agents (\(G\), \(J\), and \(F\)) that are not in the top-rank set outlined in their work. An additional key benefit of our approach is that it can immediately highlight the presence of intransitive behaviors (cycles) in general games. Worthy of remark in this dataset is the presence of a large number of cycles, several of which are identified in Fig. 13. Not only can we identify these cycles visually, these intransitive behaviors are automatically taken into account in our rankings due to the fundamental role that recurrence plays in our underlying solution concept. This is in contrast to the Elo rating (which is incapable of dealing with intransitivities), the replicator dynamics (which are limited in terms of visualizing such intransitive behaviors for large games), and Nash (which is a static solution concept that does not capture dynamic behavior).
MuJoCo soccer dataset. (a) Discrete-time dynamics. (b) Ranking-intensity sweep. (c) α-Rank results.
Example cycles in the MuJoCo soccer domain.
We next consider games wherein the inherent complexity is due to the number of players involved. Specifically, we consider Kuhn poker with \(3\) and \(4\) players, extending beyond the reach of prior meta-game evaluation approaches that are limited to pairwise asymmetric interactions6. Kuhn poker is a small poker game in which every player starts with 2 chips, antes 1 chip to play, and receives a single card face down from a deck of size n + 1 (one card remains hidden). Then the players can bet (raise/call) by adding their remaining chip to the pot, or can pass (check/fold) until all players are either in (contributed to the pot) or out (folded, passed after a raise). The player with the highest-ranked card that has not folded wins the pot. The two-player game is known to have a continuum of strategies, which could have fairly high support, that depends on a single parameter: the probability that the first player raises with the highest card65. The three-player game has a significantly more complex landscape66. The specific rules used for the three and four player variants can be found in67, [Section 4.1].
Here, our meta-game dataset consists of several (fixed) rounds of extensive-form fictitious play (specifically, XFP from68): in round 0, the payoff corresponding to strategy profile \((0,0,0)\) in each meta-game of 3-player Kuhn corresponds to the estimated payoff of each player using uniform random strategies; in fictitious play round 1, the payoff entry \((1,1,1)\) corresponds to each player playing an approximate best response to the other players’ uniform strategies; in fictitious play round 2, entry \((2,2,2)\) corresponds to each playing an approximate best response to the other players’ uniform mixtures over their round 0 strategies (uniform random) and round 1 oracle strategy (best response to random); and so on. Note, especially, that oracles at round 0 are likely to be dominated (as they are uniform random). In our dataset, we consider two asymmetric meta-games, each involving 3 rounds of fictitious play with 3 and 4 players (Figs 14 and 15, respectively).
3-player Kuhn poker (ranking conducted on all 64 pure strategy profiles). (a) Discrete-time dynamics. (b) Ranking-intensity sweep. (c) α-Rank results.
4-player Kuhn poker (ranking conducted on all 256 pure strategy profiles). (a) Discrete-time dynamics. (b) Ranking-intensity sweep. (c) α-Rank results.
Of particular note are the total number of strategy profiles involved in these meta-games, 64 and 256 respectively for the 3 and 4 player games – the highest considered in any of our datasets. Conducting the evaluation using the replicator-dynamics based analysis of6 can be quite tedious as all possible 2-face simplices must be considered for each player. Instead, here the \(\alpha \)-Rankings follow the same methodology used for all other domains, and are summarized succinctly in Figs 14c and 15c. In both meta-games, the 3-round fictitious play strategies (\((3,3,3)\) and \((3,3,3,3)\), respectively) are ranked amongst the top-5 strategies.
The meta-game we consider next involves agents generated using the Policy Space Response Oracles (PSRO) algorithm30. Specifically, PSRO can be viewed as a generalization of fictitious play, which computes approximate responses (“oracles”) using deep reinforcement learning, along with arbitrary meta-strategy solvers; here, PSRO is applied to the game of Leduc poker. Leduc poker involves a deck of 6 cards (jack, queen, and king in two suits). Players have a limitless number of chips. Each player antes 1 chip to play and receives an initial private card; in the first round players can bet a fixed amount of 2 chips, in the second round can bet 4 chips, with a maximum of two raises in each round. Before the second round starts, a public card is revealed. The corresponding meta-game involves 2 players with 3 strategies each, which correspond to the first three epochs of the PSRO algorithm. Leduc poker is a commonly used benchmark in the computer poker literature69: our implementation contains 936 information states (approximately 50 times larger then 2-player Kuhn poker), and is non-zero sum due to penalties imposed by selecting of illegal moves, see [30, Appendix D.1] for details.
We consider in Fig. 16a the Markov chain corresponding to the PSRO dataset, with the corresponding \(\alpha \)-Rank yielding profile \((0,0)\) as the top-ranked strategy, which receives 1.0 of the stationary distribution mass and essentially consumes the entire strategy space in the long-term of the evolutionary dynamics. This corresponds well to the result of6, which also concluded that this strategy profile consumes the entire strategy space under the replicator dynamics; in their approach, however, an equilibrium selection problem had to be dealt with using human-in-the-loop intervention due to the population-wise dynamics decomposition their approach relies on. Here, we need no such intervention as \(\alpha \)-Rank directly yields the overall ranking of all strategy profiles.
PSRO poker dataset. (a) Discrete-time dynamics (top 8 agents shown only). (b) Ranking-intensity sweep. (c) α-Rank strategy rankings and scores (top 8 agents shown only).
We introduced a general descriptive multi-agent evaluation method, called \(\alpha \)-Rank, which is practical and general in the sense that it is easily applicable in complex game-theoretic settings, including \(K\)-player asymmetric games that existing meta-game evaluation methods such as6,7 cannot feasibly be applied to. The techniques underlying \(\alpha \)-Rank were motivated due to the fundamental incompatibility identified between the dynamical processes typically used to model interactions of agents in meta-games, and the Nash solution concept typically used to draw conclusions about these interactions. Using the Nash equilibrium as a solution concept for meta-game evaluation in these dynamical models is in many ways problematic: computing a Nash equilibrium is not only computationally difficult21,22, and there are also intractable equilibrium selection issues even if Nash equilibria can be computed23,24,25. \(\alpha \)-Rank, instead, is theoretically-grounded in a novel solution concept called Markov-Conley chains (MCCs), which are inherently dynamical in nature. A key feature of \(\alpha \)-Rank is that it relies on only a single hyperparameter, its ranking-intensity value \(\alpha \), with sufficiently high values of \(\alpha \) (as determined via a parameter sweep) yielding closest correspondence to MCCs.
The combination of MCCs and \(\alpha \)-Rank yields a principled methodology with a strong evolutionary interpretation of agent rankings, as outlined in Fig. 17; this overarching perspective considers a spectrum of evolutionary models of increasing complexity. On one end of the spectrum, the continuous-time dynamics micro-model provides detailed insights into the simplex, illustrating flows, attractors, and equilibria of agent interactions. On the other end, the discrete-time dynamics macro-model provides high-level insights of the time limit behavior of the system as modeled by a Markov chain over interacting agents. The unifying link between these models is the MCC solution concept, which builds on the dynamical theory foundations of Conley29 and the topological concept of chain components. We provided both scalability properties and theoretical guarantees for our ranking method. Finally, we evaluated the approach on an extensive range of meta-game domains including AlphaGo1, AlphaZero3, MuJoCo Soccer5, and Poker30, which exhibit a range of complexities in terms of payoff asymmetries, number of players, and number of agents involved. We strongly believe that the generality of \(\alpha \)-Rank will enable it to play an important role in evaluation of AI agents, e.g., on leaderboards. More critically, we believe that the computational feasibility of the approach, even when many agents are involved (e.g., AlphaZero), makes its integration into the agent training pipeline a natural avenue for future work.
A retrospective look on the paper contributions. We introduced a general descriptive multi-agent evaluation method, called \(\alpha \)-Rank, which is practical in the sense that it is easily applicable in complex game-theoretic settings, and theoretically-grounded in a solution concept called Markov-Conley chains (MCCs). \(\alpha \)-Rank has a strong theoretical and specifically evolutionary interpretation; the overarching perspective considers a chain of models of increasing complexity, with a discrete-time macro-dynamics model on one end, continuous-time micro-dynamics on the other end, and MCCs as the link in between. We provided both scalability properties and theoretical guarantees for the overall ranking methodology.
