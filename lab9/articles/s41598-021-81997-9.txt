Zebrafish tracking using YOLOv2 and Kalman filter
Fish show rapid movements in various behavioral activities or associated with the presence of food. However, in periods of rapid movement, the rate at which occlusion occurs among the fish is quite high, causing inconsistency in the detection and tracking of fish, hindering the fish's identity and behavioral trajectory over a long period of time. Although some algorithms have been proposed to solve these problems, most of their applications were made in groups of fish that swim in shallow water and calm behavior, with few sudden movements. To solve these problems, a convolutional network of object recognition, YOLOv2, was used to delimit the region of the fish heads to optimize individual fish detection. In the tracking phase, the Kalman filter was used to estimate the best state of the fish's head position in each frame and, subsequently, the trajectories of each fish were connected among the frames. The results of the algorithm show adequate performances in the trajectories of groups of zebrafish that exhibited rapid movements.
Social and collective influence is a major challenge for contemporary science, being important for advances in several fields, such as in the organization and in the exchange of information1. In groups of animals, behavior has been extensively studied to assess communication among members of the group and obtain a good performance of tasks together. In this sense, there is a growing interest among researchers to assess the collective behavior of animals in order to explain their cognitive evolution2. Given this perspective, many researches were based on animal behavior, as well as the creation of bioinspired algorithms for solving optimization problems3,4 and behavioral assessment systems 5,6,7,8, employed in several areas of knowledge. Among the computer vision algorithms for assessing behavior, the methods for tracking objects are the most commonly used, as they allow a thorough assessment of the unexpected movements of different groups of animals, essential for the analysis of collective behavior 9,10,11,12,13,14,15,16,17.
Zebrafish (Danio rerio) are widely adopted as a study model in biology 9,10,11,17,18,19,20, their school of fish can represent different systems of communication and behavior. In addition, to study the individual behavior of fish in detail, tracking various objects is the most appropriate way. Most tracking systems are based on deep learning19, particle filters21, adaptive filters9,10 and others 11,12,13,18,19,22. However, to be able to track the behavior of fish for a long time, with minimal loss of identity throughout the frames, it is necessary to implement more complex systems that demand higher computational cost. Recently, a tracking system, idtracker.ai, was implemented with two convolutional networks simultaneously to improve the efficiency of animal tracking: one network used to detect when animals touch or cross each other, making the necessary correction, and another one to identify each animal. Thus, groups of zebrafish of up to 100 fish were tracked with an accuracy greater than 99.95%, in an environment with little noise2.
Usually, the analysis is based on capturing frames in videos, processing the frame images for detection and tracking of the fish school. However, some technical problems are recurrent when the evaluation of the school of fish is done automatically, with a minimum of human interference. In this regard, many difficulties can be pointed out in data processing in the following steps: detection and tracking. In detection, it is common to have problems in the automatic identification of fish, and when the barycenter is used for detection, there may be limitations in detecting individuals completely; in addition, the failure to detect can exist in times of occlusion or when the visual structure of the fish changes with sudden movements in the tank. On the other hand, tracking, which is a detection-dependent step, can present difficulties over time, where tracking can be lost due to complex fish swimming, detection errors, trajectory fragmentation and, consequently, the loss of fish identity9,10.
Several authors have proposed systems to solve these challenges, where they have created complex models of object identification, color tag detection systems14, object identification tags23, set of variables for unique identification, time complexity algorithms in contour identification24 and, detection by separating the head and body of the fish9,12,20. However, these algorithms have a high complexity of analysis and processing, which requires greater amounts of frames per second when recording videos (about 60 to 100 frames per second)9, and sometimes they are semi-automatic25, requiring manual intervention to reconnect lost tracking.
Therefore, it is important that new algorithms minimize several of the problems mentioned above. In this sense, here is proposed a method using YOLOv2 convolutional nets26,27, for delimiting the region of the fish head for optimized individual fish detection and Kalman filter for tracking multiple fish in adverse situations, solving problems such as: detection and continuous identification of fish during periods of fast swimming, analysis in low image resolutions, occlusions and minimum number of frames per second, thus showing precision in the trajectory of different quantities of fish.
To test the proposed system, we used benchmarks containing eight video sequences, six of which are video sequences related to our own experiment (D1–D6, see videos S1–S6), having a total of 8427 frames with 1920 × 1080 resolution; and two sequences of videos available from previous publications, being D7 and D8 (Romero-Ferrero et al.2; see videos S7 and S8) with a total of 6820 frames (3712 × 3712 resolution). All data sets were recorded at 30 frames. Tracking performance was compared to ground truth data. The videos had different durations and number of fish, ranging from 3 to 100 fish, and are described on Table 1. The experimental apparatus is shown in Fig. 1. The behavior of the fish was measured by the average speed, showing two conditions of movement: slow and fast (feeding period). The proposed tracking algorithm was developed using a personal computer (Intel Core i7 7700HQ CPU at 2800 8 GB RAM, Geforce gtx 1050 4 GB off-board graphics card) using Matlab R2019b software.
The architecture of the YOLOv2 network used in this article is shown in Fig. 5. In this architecture, there are 24 layers, seven layers of convolution, six layers of Relu, three layers of Max Pooling, six layers of Batch normalization and two layers of anchoring. The input layer corresponds to the network input, with the image size of 480 × 480 × 3. YOLO's convolutional layers decrease the sample by a factor of 32, obtaining the delimitations of the region of the fish head, with the convolution filter size being a 3 × 3 block and the convolution step is 1. The Max Pooling layer has the size 2 × 2, with step 2 and filling 0. All main blocks have a Relu operation layer. At the end of the block, it has two layers of anchor boxes: one for transformation and one for output. The first anchor layer transforms the output of the raw convolutional network into a format necessary to produce object detections; and the second defines the parameters of the anchor box, implementing the loss function used to train the detector31. In this way, YOLOv2 will be able to make use of the lower and upper level information, increasing the precision of the detection and location of the region of the fish head in the image.
Architecture of the proposed YOLOv2 network framework. The input image has a size of 480 × 480, and each layer is reduced by filters of up to 60 × 60. All combinations of the network architecture blocks have a convolution layer, Batch normalization and Relu, with the exception of the last layer that presents the exit through convolution and the anchor boxes. Only the three initial layers have Max pooling.
During the training, a set of 3200 image grids, 1100 image grids of videos from our experiment and 500 image grids from the database of Romero-Ferrero et al2 (approximately 1600 original size images) were initially manually labeled with the region of the fish head, being fed into the YOLOv2 network.
After the initial training, the YOLOv2 network was used to label and create the boundaries on the fish heads in new images, forming an automatic labeling process. To validate the training, a test base was established to know the assertiveness of the labeling of fish from the initial base, in case of wrong or missing labels. To improve the result of the initial training, the training base was extended with new images labeled and marked from the automatic process, but with manual corrections in the event of any error. This process reduced the time to create a training base. When the training was able to create all the delimitations of the region of the fish head correctly, the training base used was sent to the YOLOv2 network for official training.
Because the YOLOv2 networks are trained and evaluated in small resolutions (228 × 228 in the case of resnet50) and the frame size of the proposed system is 1920 × 1080 pixels, the image when resized could lose important features of the region of the fish head. The input images from the YOLOv2 network were cut by dividing the image in half. The reduction is made by creating two grids of 960 × 1080 pixels with an overlap of 30 pixels, totaling an image of 990 × 1080 pixels (Fig. 6). Then each grid is reduced to a dimension of 480 × 480pixels which is used as input to the YOLOv2 network. After the creation of the delimitations of the region of the fish head in the images meshed by the net, the process of assembling the complete image is established, using the image overlay algorithm, and the correction of duplication of the region of the fish head located on the edges of the grids (Fig. 6c). If this result is greater than the 0.4 threshold, then the same detection will be considered and, thus, a single delimitation of the region of the fish head will be assigned to the fish located on the edge of the image. This way, the detection of the delimitation of the region of the fish head can appear at the edge of the grids without loss of data, but with duplication corrected.
Example of a grid image with overlapping edges and duplication of the delimitation the region of the fish head. (a) Input image for YOLOv2 network divided into grids for the process of fish detection. (b) Assembly of grids with overlapping edges. (c) The red stripes represent the overlapping of the edges of the images during the assembly process (equivalent to 30 pixels wide). Fish were detected in the edge of grids and the correction was made using the grid intersection method presented in Eq. (1).
The YOLOv2 training protocol was based on MATLAB tools for convolutional networks, following the parameters of the convolution layers. The training process was carried out for 300 periods, using a learning rate of 0.001.
The results of the detection evaluation are presented in Table 2. The proposed method presented a superior performance of up to 0.99 of precision for all data sets in our experiment (D1–D6), thus showing that the proposed detection method based on YOLOv2 to create delimitations of the region of the fish head and assist in the calculation of the centroid, helps in the detection with different quantities of fish in the groups. Evaluating the detection in the videos, in which the fish presented slower movements (D1, D2 and D3, with an average speed of 9.42 cm/s, 7.81 cm/s and 6.44 cm/s, SI: 0.9963, 0.9883 and 0.9884, respectively), with groups of 3, 6, and 8 fish, the precision reached 1.00 and F-measure reached 0.99.
In faster movements (D6_food), during the feeding period, some losses in the detection were observed, but the algorithm was able to detect fish with good precision (0.9987), even with high occlusion frequency, a total of 715 occlusions (Table 2), and with high average speed per fish about 10.22 cm/s (Table 1). It was noticed that the occlusion is related to the fast and agglomerated swimming of the school, mainly in the feeding period (D6_food, OR: 0.0362, ODR: 0.6252, Miss ratio: 0.0337, Error ratio: 0.0013, F-measure: 0.9822, SI: 0.9653). Figure 7 shows some examples of detection and occlusions.
Fish detection. (a) Image of the complete detection of the fish head region for a group of 13 fish; (b) Examples of fish occlusion events, in some cases, there was a failure in detection.
This method uses the fish head region to calculate the centroid and detect the fish in the images. On the other hand, in other previously published works, the authors used the fish's body as a complement to the detection and identification9,10,17,32. Thus, the proposed method was able to detect fish in several situations considered difficult for the detection to remain correct, being incapable only in times of occlusion, where the fish's head was totally occluded by another fish. In addition, there was little loss of detection when the environment was noisy and challenging. Thus, it was possible to evaluate videos in different conditions, so that the method does not limit only one tested video input; it can be used in other shooting conditions.
We tested the proposed algorithm in another dataset with videos containing groups of 10 fish (video D7) and a group of 100 fish (video D8), available for free by Romero-Ferrero et al.2, referring to the idtracker.ai system, the videos had high resolution (3712 × 3712 pixels) and different environmental configuration from our experiment. The results of videos D7 and D8 were greater than 0.999 precision for both analyzed videos (Table 2). The fish were at an average speed of 8.14 cm/s and 7.49 cm/s, videos D7 and D8, respectively. The number of occlusions was relatively low, compared to the other videos analyzed in this article, thus obtaining the D7: 343 occlusions, OR: 0.0063, ODR: 0.7347, F-measure: 0.9994 and SI: 0.9988, and D8: 399 occlusions , OR: 0.0027, ODR: 0.7268, F-measure: 0.9658 and SI: 0.9328. It is observed that the increase in the quantity of fish (100 fish) did not interfere with the detection precision of the proposed algorithm.
Table 3 shows the tracking result, using the CTR and CIR metrics. The tracking is calculated similarly to the detection of a fish, calculating the percentage of the frames correctly tracked for each fish and the correct probability of all fish. The CTR describes the percentage of frames correctly tracked for each fish and, in this method, the exchange of identifications is not evaluated, but only if the tracker can follow the fish head correctly. In this study, the correct tracking of the fish in the frame was defined by the distance from the actual position of the center of the fish head in relation to the center of the tracking at a distance of up to 15 pixels, a greater distance or the absence of a tracking in the frame is considered to be an incorrect tracking. The results show a good percentage of frames correctly tracked of a single fish during a video, reaching up to 100% in the tracking when the quantity of fish is reduced (D1, CIR: 1.00, CTR: 1.00; and D2, CIR: 1.00, CTR: 0.99). It was observed that the fish loses its identification for a short period of time (D3, CIR: 0.95, CTR: 0.99 and D4, CIR: 0.96, CTR: 0.99). It is noticed that larger quantities of fish increase the amount of exchanges or losses of identification. In addition, the probability of correct identification of all fish after an occlusion is compromised with larger quantities of fish in the dataset or even with faster swimming motions (D5, CIR: 0.91, CTR: 0.98 and D6_food : CIR: 0.83, CTR: 0.97).
In this sense, it was already expected that in the feeding period, the tracking of fish would be lost, as there were many occlusions and totally unexpected movement. The biggest tracking losses were in videos D5 and D6_food. The tracking of videos D7 and D8 performed well, obtaining a CTR value of up to 0.99 in tracking fish (D7, CIR: 0.9375, CTR: 0.9970 and D8, CIR: 0.9285 and CTR: 0.9983). It is observed that the algorithm was able to track fish from another dataset, where the videos were recorded with a higher resolution than our images (Table 1) and experimental environment different from that proposed here. In addition, it is notable that the spread of errors decreases as occlusions happen less frequently when the fish swim in the tank.
Figure 8 shows a visual example of tracking in three video sequences (D1, D2 and D6_food).
Result of tracking different groups of fish. (a) Tracking of group D1 with 3 fish and (b) group D2 with 6 fish, both in a state of slow motions, (c) group D6_food with 13 fish are in the feeding period (fast motions). The fish swim with greater agitation and agglomeration in the center of the aquarium, a space where the food was concentrated.
The method proposed in this work combined two techniques to detect and track fish schools: the YOLOv2 network and the Kalman filter. Previous tracking methods showed applications of fish swimming in shallow waters and slow motions9,10,17,19,32. Here, we test the proposed method in adverse situations such as the feeding period, which is widely used for training and conditioning fish. It was noticed that the quantity and agitation of the school of fish are important factors for the increase in occlusion, which makes detection difficult most of the time.
However, through this method, we showed that detection and tracking were obtained around 99% in low and high resolution images, with variation in the quantity of fish (up to 100 fish). High resolution images, such as the images available from idtracker.ai2, proved to be favorable for successful screening. In addition, it was observed that the spread of errors decreases when the fish have little occlusion.
In some methods, the fish's entire body segmentation is used to detect and identify the fish9,10,17,32, however we created a delimitation of the region of the fish head to facilitate detection, this approach has been presented in other works, but using different techniques9,19. The performance of the YOLOv2 network improves as training is increased, improving the result of the delimitation of the region of the fish head.
The fish was detected by calculating the centroid, based on the identification of the region of the fish head by YOLOv2. The Kalman filter was used to adjust the centroid, estimating the fish's position between frames, when there was loss of detection. A fish changes from one position to another quickly, and a method for estimating the position in the tank may fail to identify the fish's current position. An adjustment of the cost function has considerably improved the prediction of the state of the fish, making it possible to create trajectories. In order to establish the lost trajectories, the route was reconnected using the shortest distance. In this study, a much higher frame rate is not necessary, we used a frame rate considered low at 30 frames per second, and we were able to track the fish efficiently. Higher rates can compromise the algorithm's run time.
Although we were able to correctly identify fish at around 0.83 (CIR) in the feeding period, where fish can have rapid movements and many variations in the direction of movement, it is still a limitation when the number of occlusions between fish is high, as there may be an exchange of fish identification. In addition, long occlusions, which are greater than 15 frames, may have a greater chance of exchanging fish identification, due to the lack of an identification step.
Thus, the inclusion of an identification step, could improve the tracking of the fish in the entire route in the tank, especially after long occlusions, reducing the chances of propagating errors, compromising the performance of the method. The YOLOv2 network when trained with different classes can detect multiple objects from the same or different classes in the same image. When used as a unique identification method for several individuals of the same species in the same scene, a better performance is achieved in terms of speed and accuracy in carrying out the task. In idtracker.ai2, a fish identification step is used, without the need for an additional method to associate the detection of objects between frames, generating greater accuracy in tracking, especially during fish occlusions. A new approach could be attributed to our work, which includes a fish identification step to further improve tracking.
