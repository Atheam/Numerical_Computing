Automated detection of cribriform growth patterns in prostate histology images
Cribriform growth patterns in prostate carcinoma are associated with poor prognosis. We aimed to introduce a deep learning method to detect such patterns automatically. To do so, convolutional neural network was trained to detect cribriform growth patterns on 128 prostate needle biopsies. Ensemble learning taking into account other tumor growth patterns during training was used to cope with heterogeneous and limited tumor tissue occurrences. ROC and FROC analyses were applied to assess network performance regarding detection of biopsies harboring cribriform growth pattern. The ROC analysis yielded a mean area under the curve up to 0.81. FROC analysis demonstrated a sensitivity of 0.9 for regions larger than \({0.0150}\,\hbox {mm}^{2}\) with on average 7.5 false positives. To benchmark method performance for intra-observer annotation variability, false positive and negative detections were re-evaluated by the pathologists. Pathologists considered 9% of the false positive regions as cribriform, and 11% as possibly cribriform; 44% of the false negative regions were not annotated as cribriform. As a final experiment, the network was also applied on a dataset of 60 biopsy regions annotated by 23 pathologists. With the cut-off reaching highest sensitivity, all images annotated as cribriform by at least 7/23 of the pathologists, were all detected as cribriform by the network and 9/60 of the images were detected as cribriform whereas no pathologist labelled them as such. In conclusion, the proposed deep learning method has high sensitivity for detecting cribriform growth patterns at the expense of a limited number of false positives. It can detect cribriform regions that are labelled as such by at least a minority of pathologists. Therefore, it could assist clinical decision making by suggesting suspicious regions.
Prostate cancer is one of the most common cancer types in men: about one man in 9 is diagnosed with prostate cancer in his lifetime1. Histological image analysis of biopsy specimens is generally considered the reference standard for detection and grading of prostate cancer. The Gleason grading system is often used in practice to evaluate the severity of prostate cancer. The Gleason system distinguishes five basic architectural growth patterns, numbered Gleason grade G1 to G5. Presently, combinations of prevalent growth patterns are usually considered which is reflected in the Gleason Score and Grade Group (Table 1). However, in spite of such updates the current system is still associated with high inter-observer variability2. For example, the classification between Grade Group 2 and 3 (Gleason Score \(3+4=7\) and \(4+3=7\)) is often subject to disagreement among pathologists. The classification between these two Grade Groups is highly relevant since it influences therapeutic decision-making. Actually, each individual Gleason grade is a collection of different growth patterns (Fig. 1). Particularly, Gleason grade 4 comprises glands forming cribriform, glomeruloid, ill-defined, fused, and complex fused growth patterns3,4. Unfortunately, the disagreement among pathologists is also relatively high regarding the sub-type classification.
In a recent inter-observer study, high consensus, i.e. 80% agreement among 23 pathologists, was reported on 23% of the cribriform cases, but rarely on fused and never on ill-defined patterns5. At the same time it was found that presence of cribriform growth patterns in prostate cancer imply a poor prognosis6,7,8. The cribriform growth pattern could therefore be an important prognostic marker and its detection might add valuable information on top of the Gleason grading system.
Many clinical decisions have to be made during

the treatment of prostate cancer patients, often by multidisciplinary teams or tumor boards. These decisions are complex due to the increasing number of available parameters e.g. from radiological imaging, pathology and genomics. There is a clinical need for technology that enables objective, reproducible quantification of imaging features. Specifically, automatic qualification of the biopsies regarding the Gleason grade and biomarkers derived from automated detection of cribriform glands would add objective parameters in a clinical decision support algorithm. Such automated detection tools can also bring visualization support to aid clinical decision-making.
We propose a method to automatically detect cribriform glands in prostate biopsy images. As the annotation of cribriform glands is subject to intra- and inter-observer variability, erroneous cases will be re-evaluated by the original annotators and the algorithm’s performance will be compared against the assessments of a large group of pathologists.
Engineered feature based machine learning approaches were used to identify stroma, benign and cancerous tissue for radical prostatectomy tissue slides9. Automatic Gleason grading were proposed as well using multi-expert annotations and multi-scale features based methods10. Additionally, deep learning methods have proven useful in digital pathology for various tasks such as detection and segmentation of glands, epithelium, stroma, cell nuclei and mitosis11. This is relevant as such tissue segmentations can be a first step to a more detailed characterization. More recently, a variety of CNN-based methods were also used to classify prostate cancer tissue. These approaches differed regarding the type of histological images: Tissue Micro-Arrays (TMAs)12 and Whole Slide Images (WSIs)13,14 acquired after radical prostatectomy versus WSIs obtained from prostate needle biopsies. Biopsy interpretation is challenging, though, due to the narrow tissue width since typically a needle diameter of around 1 mm is used. Importantly, assessment of needle biopsies can have impact on management of individual patients. Previously, segmentation and classification methods for automatic processing of prostate needle biopsies were developed to detect malignant tissue15,16, as well as for partial17,18 and full Grade Group classification19,20. Automated detection of cribriform growth patterns in prostate biopsy tissue has, to the best of our knowledge, not yet been studied. Indeed, Gertych et al.21 proposed a CNN combined with a soft-voting method to automatically distinguish four growth patterns including the cribriform growth pattern, but this was applied to lung tissue samples only. Moreover, it was stated that the method had only moderate recognition performance (F1-score=0.61) with regards to the cribriform growth pattern.
In order to assist pathologists and support clinical decision making, we aim to introduce a method for automatic detection of cribriform growth patterns. In summary, this paper presents the following contributions:
Cribriform growth patterns are automatically detected and segmented from tissue slides obtained from prostate needle biopsies.
Annotations of erroneous cases are re-considered to account for intra-observer variability.
Algorithm performance is compared against assessments by a large group of pathologists.
(top) Examples of different biopsy tissues with glands classified as Gleason grade 3 or 4. (bottom) Deep convolutional neural network architecture: a biopsy patch image is fed into the network and the output after a softmax normalization is composed of 7 segmentation maps 32 times smaller than the input. The network is composed of 6 consecutive conv blocks. A conv block consists of 2 consecutive convolutions of input features (f feature maps of height h and width w) with a squeeze-and-excitation block and a residual connection. Downsampling is done using strided convolutions with a kernel 2x2, after which batch normalization (BN) and an activation rectifier linear unit (ReLU) are applied.
Figure 2 shows the ROC curves representing the cribriform detection sensitivity as a function of false positive rate per biopsy. Complementary, it shows the FROC curves of the cribriform detection sensitivity per annotation as a function of the mean number of false positive detections in biopsy.
In both figures, the dashed curves (and associated shaded areas) depict the mean performance (and corresponding standard deviation) of the ensemble network between the 8 folds. The green curve collates results based on all cribriform prediction regions, whereas the blue curve considers only results of predicted regions larger than \({0.0150}\,\hbox {mm}^{2}\).
The area under the curve (AUC) of the ensemble network in the ROC curve was on average 0.80 with all regions and 0.81 for regions larger than \({0.0150}\,\hbox {mm}^{2}\). In order to compare the impact of the ensemble network with regards to the 16 individual networks, Table 3 shows the AUC of the network ensemble and mean AUC of the 16 networks across the 8 folds.
Several representative example images with predictions and annotations are presented in Fig. 3.
(left) Receiver operating characteristic (ROC) curves showing cribriform detection sensitivity per biopsy as a function of the false positive rate (i.e. 1-specificity). Dashed lines represent mean ROC curves from the ensemble networks of the 8 folds; the associated shaded areas represent the corresponding standard deviations. The green curve concerns all predicted cribriform regions, the blue curve was based on predicted regions larger than \({0.0150}\,\hbox {mm}^{2}\). (right) Free-response receiver operating characteristic (FROC) curves of the cribriform detection sensitivity per annotation as a function of the average number of false positive predictions per biopsy. The green dot indicates, for the mean FROC curve with all prediction regions, the cut-off probability of network output (\(p=0.5\)) conveniently chosen for the re-evaluation experiment (see Re-evaluation study section).
Biopsy slides with overlays showing predicted regions by the ensemble network as well as annotations (serving as reference). Light blue indicates true positive predictions; green are false negative regions; dark blue are false positive predictions. The scale of the images differs merely for illustration purposes.
In order to extract the false positive and negative regions of the ensemble network, we applied a cut-off to the cribriform prediction probability. We conveniently chose the cut-off to 0.5 (Fig. 2, right) in order to have a moderate amount of false positive regions to annotate, given the limited time the pathologists could allocate to this task. Applying the cut-off to the cribriform prediction probability yielded in total 632 false positive and 25 false negative cribriform region patches.
During the re-evaluation, the pathologists gave ‘cribriform’ as the first label to \(9\% (=59/632)\) of the false positive patches. Furthermore, the pathologists indicated ‘cribriform’ as the first or second label (the ‘doubtful’ cases) to \(20\%\) of false positive patches.
At the same time, upon re-evaluation the pathologists did not indicate ‘cribriform’ as the first label in \(44\% (=11/25) \) of the false negative cases. Furthermore, the pathologists did not indicate ‘cribriform’ as the first nor as the second label in \(16\%\) of those cases.
For \(71\% (=468/657\)) of the wrongly classified patches (taking false positives and negatives together), the originally given label was identical to the first label during re-evaluation. Furthermore, in \(48\%\) of the false positive patches and \(36\%\) of the false negative patches, no second label was given.
The median confidence level was 4 (highly confident) for patches with same labels during the initial annotation and the first annotation of the re-evaluation. The median confidence level was 2 for patches labelled differently as such. Furthermore, confidence level 4 was given to \(51\%\) of the false positive patches and \(28\%\) of the false negative patches during re-evaluation.
An overview of the initial annotations and first label during re-evaluation of the false positives and false negatives is contained in the confusion matrix in Fig. 4. Additionally, the figure shows examples of the false positive and negative cases including details on the annotations by the pathologist.
(left) Confusion matrix showing initial label versus first label during re-evaluation attributed by the pathologists to false positive and false negative patches. (right) Example false positive and false negative cases re-evaluated by the pathologists. Blue indicates a false positive detection and green indicates a false negative annotation. (a) False positive patch initially annotated as ‘fused’ and given ‘cribriform’ as the first label during re-evaluation. (b) False positive patch initially annotated as ‘fused’ and given ‘complex fused’ as the first label and ‘cribriform’ as the second label during re-evaluation. (c) False negative patch initially annotated as ‘cribriform’ and ‘fused’ during re-evaluation. (d) False negative patch annotated as ‘cribriform’ both during initial annotation and re-evaluation. Observe that the automatic segmentations (top) are coarse because the output of the neural network is 32 times smaller than the input; annotations (bottom) were manually drawn on the originals and are therefore smoother.
Figure 5 shows the FROC curve of the ensemble network generated based on the evaluation set of the training data by varying the probability threshold. In order to compare the performance of the ensemble network to the assessments by the 23 pathologist we applied three probability thresholds/cut-offs: (1) 0.0125 corresponding to the highest attained sensitivity on the evaluation set; (2) 0.1 at which level \(95\%\) sensitivity is reached; (3) 0.5 at which level \(85\%\) sensitivity is reached.
Top-right and bottom-right charts in Fig. 5 show the number of regions predicted as cribriform and not cribriform respectively by the neural network as a function of the percentage of pathologists annotating these images as cribriform.
Observe that there are 9, 3 and 0 regions labelled as cribriform by the network applying the thresholds at 0.0125, 0.1 and 0.5, respectively, which no pathologist annotated as cribriform. These could be considered as false positive cases. Furthermore, the 21, 27 and 30 regions with the same thresholds labelled as not-cribriform by the network nor labelled as cribriform by any pathologist could be considered true negatives. The bottom-right chart in Fig. 5 demonstrates that with the cut-off at 0.0125 all the images annotated as cribriform by more than \(30\%\) (\(\ge \) 7/23) of the pathologists are predicted as cribriform by the neural network. Increasing the threshold to 0.1 and 0.5 leads to more regions not classified as cribriform and simultaneously less false positives. It may be noted that for 63% (=19/30) of the cribriform cases, less than 60% of the pathologists agreed regarding the labeling.
The average of Cohen’s kappa coefficient across all paired pathologist labelings is 0.62. The average of Cohen’s kappa coefficient between our method and each pathologist is 0.29, 0.36 and 0.39 at cut-offs of 0.0125, 0.1 and 0.5, respectively.
Figure 6 shows examples images for varying agreements between the pathologists on which the cribriform regions detected by the neural network are overlaid.
(left) Free-response receiver operating characteristic (FROC) curve of the cribriform detection sensitivity per annotation as function of the average number of false positive predictions per biopsy. This curve was resulted from the evaluation set using the ensemble of neural networks trained for the inter-observer study. The green dot corresponds with a probability cut-off at 0.0125, at which point the highest sensitivity is attained. The blue and orange dots correspond to probability cut-off at 0.1 and 0.5 yielding \(95\%\) and \(85\%\) sensitivity, respectively. (top-right) Numbers of images predicted as cribriform by the neural network as a function of the percentage of pathologists labeling these images as cribriform. (bottom-right) Numbers of images not predicted as cribriform by the neural network as a function of the percentage of pathologists labeling these images as cribriform.
Cribriform growth pattern detection on images from the dataset of Kweldam et al.5. The yellow contours delineate the regions that were assessed by the pathologists. Green, blue and orange colors correspond to detection of cribriform regions at probability thresholds of 0.0125, 0.1 and 0.5 respectively. (a,b) Images annotated as cribriform by 22 and 23 pathologists, respectively. (c,d) Images annotated principally as ill-defined and as G3, respectively (no cribriform annotation). (e,f) Images annotated as cribriform by 22 and 8 pathologists, respectively. Image (f) was annotated as fused by 13 pathologists. Note that the yellow contours for images (e) and (f) is at the image border.
We proposed a method to automatically detect and localize G4 cribriform growth patterns in prostate biopsy images based on convolutional neural networks. In order to improve the detection of cribriform growth patterns the network was trained to detect other growth patterns (complex fused, glomeruloid, ...) as well. Furthermore, to enhance the stability of the prediction, an ensemble of networks was trained after which the average prediction was used.
The ensemble networks focusing on regions larger than \({0.0150}\,\hbox {mm}^{2}\) reached a mean area under the curve of 0.81 regarding detection of biopsy images harboring a cribriform region. The FROC analysis showed that achieving a sensitivity of 0.9 for regions larger than \({0.0150}\,\hbox {mm}^{2}\) goes at the expense of on average 7.5 false positives per biopsy.
The evaluation of the 16 individual neural networks show marked variation in AUC value. This could be an indication that the training data order (stochastic variation) influences the performance of the network. Our solution to cope with this was to apply the ensemble of neural networks. An alternative solution could be hard negative mining by prioritizing ’difficult’ regions during training. In this way, the training process would more frequently present patches with large classification discrepancy across training iterations and thus would reduce the influence of the training data order. On the other hand, the AUC variation between the folds of the cross-validation has an even higher standard deviation than the stochastic variation on the ROC and FROC curves. Of course, yet another solution would be to acquire a larger training set, to take into account the heterogeneity of the data in a better way.
During the evaluation of the method, the variations among pathologists regarding cribriform growth pattern recognition was also studied. In particular, false positive and false negative patches from the ensemble classifier were re-evaluated by the same pathologists.
The evaluation of the method with the original annotations showed that sensitive cribriform region detection can be done, but at the expense of a high number of false positives. However, the re-evaluation study demonstrated that up to 20% of the false positive detections could actually be cribriform regions. Concurrently, it showed that up to 44% of the false negatives might not be cribriform regions.
We also tested the ensemble network on a dataset annotated by 23 pathologists to put its performance into perspective regarding inter-observer variability. In 63% of the cases, less than 60% of the pathologists agreed regarding the cribriform labeling. Using a probability cut-off at 0.0125 (corresponding to the highest sensitivity in the training set) all images annotated as cribriform by at least \(30\%\) of the pathologists were also predicted as cribriform by the neural network. In other words, the network is rather conservative in classifying regions as cribriform even with a low percentage of agreement, which is opposed to detecting only regions for which there is large agreement. This is also reflected by the Cohen’s kappa which showed higher agreement amongst pathologist than between our method and the pathologists. As such, with the cut-off at 0.0125, the network is more inclusive than the consensus of pathologists. This could be clinically relevant as preferably no potentially cribriform region should be missed by the automated detection algorithm at this stage.
Some fused and tangentially sectioned glands were falsely labelled as cribriform. In practice, biopsies are cut at three or more heights giving additional information to the pathologist, while we only used one level in the current study. We believe that the performance for recognition of cribriform architecture or grading in general can be improved if information of different levels of the same biopsy specimen can be registered and integrated in the future. Furthermore, due to its relatively large size, cribriform architecture may not be visualized in its entirety in biopsies, which is different from the situation in operation specimens from radical prostatectomies. Therefore, optimal training sets for cribriform pattern should be slightly different for biopsy and prostatectomy specimens.
There were typical false positive cribriform detections. First, as malignant glands are not properly attached to the surrounding stroma, tearing may happen during tissue processing. Figure 6c,d show resulting retraction and slit-like artifacts surrounded by cells with clear cytoplasm. The resulting background could be mistaken for cribriform lumina by the network. Second, in Fig. 3c,f, some regions in non-labelled tissue have complex anastomosing glands, not meeting the criteria for cribriform growth. Finally, in the latter figure, the background seems confused by the network for cribrifrom lumina. Since these areas are but a small part of the total area of non-labelled tissue it could be that the network might not have seen sufficient examples of such patterns during the training.
The latter observation signifies the importance of diversity in the training data also for our application. Inclusion of healthy tissue simultaneously with rare tissues such as G5, mucinous, and perineural structures are indispensable in the training set. For similar reasons, an important direction for future work could be to particularly focus on multi-center data. Also, annotations from multiple pathologists might help to build a more detailed probabilistic model and cover the variability from large consensus to large ambiguity. Previously, such approaches were proposed by Nir et al.10 for Gleason grading and by Kohl et al.28 for segmentation of lung abnormalities.
A limitation of our study is that a re-evaluation by the pathologists of true positives and true negatives cases is lacking. While our re-evaluation analysis indicates that some of the false positives and false negatives cases may not necessarily be false, it is likely that reassessing all the samples would also turn some true positives and true negatives patches into false positives and false negatives, respectively. Furthermore, as it is stated in Kweldam et al.5, the dataset from the inter-observer study has been deliberately chosen to be difficult which may lead to more disagreement between pathologists than with biopsy analyses during daily clinical practice. The performance of the neural network is also impacted by this as such uncommon cases were not present in the training. Also, despite its predictive value, no global consensus exists yet on the definition of cribriform architecture and its delineating features from potential mimickers. The pathologists who annotated our dataset have however shown statistically significant correlation of cribriform pattern with clinical outcome in large biopsy and operation specimens cohorts, clinically validating the criteria used in this study6,8,29.
