A detailed characterization of complex networks using Information Theory
Understanding the structure and the dynamics of networks is of paramount importance for many scientific fields that rely on network science. Complex network theory provides a variety of features that help in the evaluation of network behavior. However, such analysis can be confusing and misleading as there are many intrinsic properties for each network metric. Alternatively, Information Theory methods have gained the spotlight because of their ability to create a quantitative and robust characterization of such networks. In this work, we use two Information Theory quantifiers, namely Network Entropy and Network Fisher Information Measure, to analyzing those networks. Our approach detects non-trivial characteristics of complex networks such as the transition present in the Watts-Strogatz model from k-ring to random graphs; the phase transition from a disconnected to an almost surely connected network when we increase the linking probability of Erdős-Rényi model; distinct phases of scale-free networks when considering a non-linear preferential attachment, fitness, and aging features alongside the configuration model with a pure power-law degree distribution. Finally, we analyze the numerical results for real networks, contrasting our findings with traditional complex network methods. In conclusion, we present an efficient method that ignites the debate on network characterization.
Understanding how networks arrange their connections (structure), and how the information flows through their nodes (dynamics), is a breakthrough for many scientific fields that rely on network science to assess all kinds of phenomena. Recently, Information Theory methods gained the spotlight because of their ability to create a more quantitative and robust characterization of complex networks, as an alternative to traditional methods. Standard quantifiers such as Shannon Entropy and Statistical Complexity were adapted to network analysis, providing a different perspective when evaluating networks1.
The quantification of systems with multidimensional measures, in particular, a 2D representation or “plane representation” formed by Information Theory-based quantifiers is extensively applied to time series analysis and characterization. For instance, Rosso et al.2 employed the time causal Entropy-Complexity plane to distinguish stochastic from deterministic systems. The Entropy-Complexity plane is comprised of measures of entropy (\( {\mathcal H} \)) and Statistical Complexity (\({\mathscr{C}}\)). Two pieces of information are required to calculate \({\mathscr{C}}\), namely the information content and the disequilibrium (\({\mathscr{Q}}\)) of the system. To evaluate the Statistical Complexity, we use the entropy as a measure of the information content, and the disequilibrium is expressed by the divergence between the current system state and an appropriate reference state. The calculation of these quantifiers requires the use of a proper probability distribution that represents the system under study. In the case of time series, the distribution derived from the symbolization proposed by Bandt and Pompe3 has been successfully used to capture the intrinsic time causality behavior of the underlying system (see Supplemental Material). The Shannon Entropy is a global disorder measure commonly used in many applications of the Information Theory field and the Entropy-Complexity plane. It is relatively insensitive to substantial changes in the distribution taking place in a small-sized region of the space. For these reasons, the Shannon Entropy is referred to as a global measure. The Statistical Complexity (\({\mathscr{C}}\)), when defined as a divergence in the space of entropies, is also a global measure. Alternatively, the Fisher Information Measure (\( {\mathcal F} \)) can be interpreted as a measure of the ability to estimate a parameter, as the amount of information that can be extracted from a set of measurements, and also, a measure of the state of disorder of a system or phenomenon4. The Fisher Information Measure (\( {\mathcal F} \)) is a local measure as it is based upon the gradient of the underlying distribution, being, thus, significantly sensitive to even tiny localized perturbations.
Recently, the Entropy-Complexity plane was extended and used in the context of complex networks. In ref. 1, the authors showed that networks of the same category tend to cluster into distinct regions. To calculate the two required quantifiers, \( {\mathcal H} \), and \({\mathscr{C}}\), for complex networks, the authors used the probability distribution of a random walker traveling between two nodes to represent the topological properties of the network. Based on this distribution, they calculated the Shannon Entropy and the Statistical Complexity. For the evaluation of the disequilibrium, they used the Jensen-Shannon divergence between the actual network and random networks and used the last as a reference model. To obtain this divergence it is necessary to average several random networks with the same number of nodes, which is typically time-consuming. They demonstrated the applicability of their proposal to families of Random Erdős-Rényi5, Small-World Watts-Strogatz6, and Scale-Free Barabási-Albert7 networks. However, this plane presents several limitations, as regularly, the random networks overlap with all the other models, creating confusion and misleading the conclusions when evaluating the network’s features.
In this work, we propose the Fisher information quantifier, more sensitive to a local relationship between nodes, as a measure of network disorder. Alongside, we suggest the use of the Shannon-Fisher plane as an alternative to the Entropy-Complexity plane for network characterization. Our approach does not require the calculation of a divergence to a reference model, which decreases the computational burden. We analyze two different groups of networks: synthetic and real-world networks.
Complex networks have many faces, thus attempting to label them considering a single network property may be misleading. Real networks have many components and distinct interactions among them, for example, a scale-free network may have peripheral communities that lead to small-world structure. Our proposal quantifies network structure and dynamics, considering a simplified plot. We show consistent results with other network features when this methodology is applied to synthetic networks.
The Shannon-Fisher plane enhances our ability to evaluate complex networks:
The transition that the Watts-Strogatz model exhibits in between k-ring and random graphs, leading us to define the small-world region;
The two distinct regimes for the Erdős-Rényi model when reaching the critical linking probability;
The three regimes for the non-linear preferential attachment on scale-free networks and distinct growth models, which transits between random, scale-free and condensed networks;
The behavior of the fitness model when we consider a uniform distribution for the fitness of each node, and how it has similar features to the Barabási-Albert model;
The effect of aging for scale-free networks and how the aging exponent can control the system’s behavior in the same manner to what happens with the non-linear preferential attachment;
And finally, how we can generate networks with a pure power-law considering distinct degree exponent.
The evaluation of real networks gave us a peek into the real world and its deceitful aspects. Our method succeeds in characterizing most of the real networks in comparison with synthetic networks, even though a few examples showed unexpected behaviors that will be widely explored further. That said, our proposal is not the perfect fit for labeling networks as a small-world or scale-free, but it opens a world of possibilities when evaluating information spread, network robustness, or controllability. Our approach allows identifying distinct interactions in real networks, observing how they transit within the Shannon-Fisher plane and comparing how they affect other network features.
We assume a graph G(V, E), where V is the set of nodes and E is the set of links (edges) as a suitable model of a network. The graph is represented by an adjacency matrix A with dimension N × N, N being the number of nodes in the network, where aij = 1 if a link exists between nodes i and j, otherwise, aij = 0. We consider undirected, unweighted, and without the presence of loops graph, i.e., simple unweighted graphs. Hence, their adjacency matrices have the main diagonal aii = 0, ∀i = 1, …, N, and A = AT. The node degree ki is calculated by \({k}_{i}=\sum _{j}\,{a}_{ij}\), therefore, 0 ≤ ki ≤ N − 1.
Network Entropy is based on the classical Shannon Entropy for discrete distributions. Small8 proposed a measure of Network Entropy based on the probability that a random walker goes from node i to any other node j. This probability distribution P(i) is defined for each node i and has entries
It is easy to observe that \({\sum }_{j}{p}_{i\to j}=1\) for each node i.
Based on the probability distribution P(i), the entropy for each node can be defined as
with \({{\mathscr{S}}}^{(i)}=0\) if node i is disconnnected.
After calculating the entropy for each node, we then calculate the normalized node entropy by
Finally, the normalized Network Entropy is calculated averaging the normalized node entropy over the whole network as
The normalized Network Entropy is maximal \( {\mathcal H} =1\) for fully connected networks, since pi → j = (N − 1)−1 for every i ≠ j and the walk becomes fully random, i.e., jumps from node i any other node j are equiprobable. The walk becomes predictable in a sparse network because it limits the possibility of jumps. The sparser the network, the lower becomes its Network Entropy.
The normalized Network Entropy \( {\mathcal H} \), hence, quantifies the heterogeneity of the network’s degree distribution, with lower values for nodes with lower degrees and higher values for nodes with higher degrees. For example, peripheral nodes present lower \({ {\mathcal H} }^{(i)}\) than hubs. Entropy, thus, ranges from \( {\mathcal H} \to 0\) (sparse networks) to \( {\mathcal H} \to 1\) (fully connected networks).
The normalized Fisher Information Measure (FIM)9 for a node i is given by
The normalized network Fisher Information Measure is given by
If the system under study is in a very ordered state, i.e., a sparse network, almost all pi → j values are zeros, we have Shannon Entropy \( {\mathcal H} \to 0\) and normalized Fisher’s Information Measure \( {\mathcal F} \to 1\). On the other hand, when a very disordered state represents the system under study, that is when all pi → j values are similar, we obtain \( {\mathcal H} \to 1\) and \( {\mathcal F} \to 0\). We can then define a Shannon-Fisher plane, which can also be used to characterize Complex Networks.
